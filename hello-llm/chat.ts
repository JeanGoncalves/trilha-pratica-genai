import { config } from 'dotenv';
import readline from 'readline';
import { Ollama } from 'ollama';
import chalk from 'chalk';

config();

const ollama = new Ollama();

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
});

async function chat(): Promise<void> {
    console.log(chalk.green('üí¨ Chatbot com Ollama iniciado! Digite "sair" para encerrar.\n'));
    
    const perguntarContinuo = (): void => {
        rl.question(chalk.yellow('Voc√™: '), async (question) => {
            if (question.toLowerCase() === 'sair') {
                console.log(chalk.green('\nObrigado por usar o chatbot! At√© mais! üëã'));
                rl.close();
                return;
            }

            try {
                console.log(chalk.gray('\nü§ñ Pensando...\n'));
                const response = await ollama.chat({
                    model: 'llama3.2',
                    messages: [{ role: 'user', content: question }],
                });
                
                console.log(chalk.blue(`Bot: ${response.message.content}\n`));
            } catch (error) {
                console.error(chalk.red('‚ùå Erro ao processar a pergunta:'), error);
            }
            
            // Continua o loop para a pr√≥xima pergunta
            perguntarContinuo();
        });
    };

    perguntarContinuo();
}

chat(); 