# Hello LLM

Um chatbot simples usando Ollama para executar modelos de linguagem localmente e de forma gratuita.

## Pr√©-requisitos

- Node.js e npm
- Ollama instalado no sistema

## Instala√ß√£o

1. Instale o Ollama:
```bash
brew install ollama
```

2. Inicie o servi√ßo do Ollama:
```bash
brew services start ollama
```

3. Baixe um modelo (exemplo: Llama 3.2):
```bash
ollama pull llama3.2
```

4. Instale as depend√™ncias do projeto:
```bash
npm install
```

## Como usar

### Vers√£o simples (uma pergunta por vez):
```bash
npm start
# ou
npx ts-node index.ts
```

### Vers√£o interativa (m√∫ltiplas perguntas):
```bash
npm run chat
# ou
npx ts-node chat.ts
```

Digite sua pergunta e pressione Enter para obter uma resposta. Na vers√£o interativa, digite "sair" para encerrar.

## Exemplo da interface

```
üí¨ Chatbot com Ollama iniciado! Digite "sair" para encerrar.

14/07/2025, 17:15 Voc√™: Ol√°, como voc√™ est√°?

14/07/2025, 17:15 ü§ñ Pensando...

14/07/2025, 17:15 ü§ñ Bot: Estou funcionando corretamente, obrigado por perguntar! Como posso ajud√°-lo hoje?
```

## Modelos dispon√≠veis

Voc√™ pode usar diferentes modelos alterando o valor do `model` no c√≥digo:

- `llama3.2` (padr√£o)
- `llama3.1`
- `codellama`
- `mistral`
- `gemma`

Para ver todos os modelos dispon√≠veis:
```bash
ollama list
```

Para baixar um novo modelo:
```bash
ollama pull nome-do-modelo
```

## Funcionalidades

- **Interface colorida**: Respostas em azul, prompts em amarelo, sistema em verde
- **Timestamp**: Mostra data e hora brasileira (formato DD/MM/AAAA, HH:MM)
- **Sistema de contexto**: Mant√©m hist√≥rico da conversa (vers√£o chat)
- **Prompt personalizado**: Instru√ß√µes espec√≠ficas para o assistente

## Vantagens do Ollama

- **Gratuito**: Todos os modelos s√£o gratuitos
- **Privacidade**: Roda localmente, seus dados n√£o saem do seu computador
- **Sem limites**: N√£o h√° limites de uso ou quotas
- **Offline**: Funciona sem internet ap√≥s baixar o modelo 